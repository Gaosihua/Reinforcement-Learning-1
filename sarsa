import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt


#Sarsa算法的具体实现
#环境是一个向上方吹风的格子世界，每次行动后会被风往上吹移动




WORLD_HEIGHT = 7
WORLD_WIDTH = 10

WIND = [0,0,0,1,1,1,2,2,1,0]

ACTION_UP = 0
ACTION_DOWN = 1
ACTION_LEFT = 2
ACTION_RIGHT = 3

#探索概率
EPSILON = 0.1

#步长/学习率
ALPHA = 0.5

#每一步的奖励
REWARD = -1

START = [3,0]
GOAL = [3,7]
ACTIONS = [ACTION_UP,ACTION_DOWN,ACTION_LEFT,ACTION_RIGHT]

def step(state,action):
    i,j = state
    if action == ACTION_UP:
        return [max(i - 1 - WIND[j],0),j]
    elif action == ACTION_DOWN:
        return [max(min(i+1-WIND[j],WORLD_HEIGHT-1),0),j]
    elif action == ACTION_LEFT:
        return [max(i - WIND[j],0),max(j-1,0)]
    elif action == ACTION_RIGHT:
        return [max(i - WIND[j],0),min(j+1,WORLD_WIDTH-1)]
    else:
        assert False

#play for an episode
def episode(q_value):
    time = 0
    state = START

    # if np.random.rand() < EPSILON:
    if np.random.binomial(1,EPSILON) == 1:
        action = np.random.choice(ACTIONS)
    else:
        values_ = q_value[state[0],state[1],:]
        action = np.random.choice([action_ for action_,value_ in enumerate(values_) if value_ == np.max(values_)])

    while state != GOAL:
        next_state = step(state,action)
        if np.random.binomial(1,EPSILON) == 1:
            next_action = np.random.choice(ACTIONS)
        else:
            values_ = q_value[next_state[0],next_state[1],:]
            next_action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])

        #update
        q_value[state[0],state[1],action] += ALPHA * (REWARD + q_value[next_state[0],next_state[1],next_action] -
                                                      q_value[state[0],state[1],action])
        state = next_state
        action = next_action
        time += 1
    return time

def sara():
    q_value = np.zeros((WORLD_HEIGHT,WORLD_WIDTH,4))
    episode_limit = 500

    steps = []
    ep = 0

    while ep < episode_limit:
        steps.append(episode(q_value))
        print('episode %d is completed' %ep)
        ep += 1


    plt.plot(np.arange(1,len(steps)+1),steps)
    plt.xlabel('episodes')
    plt.ylabel('steps')

    plt.savefig('./sarsa.png')
    plt.close()

    optimal_policy = []
    for i in range(0,WORLD_HEIGHT):
        optimal_policy.append([])
        for j in range(0,WORLD_WIDTH):
            if [i,j] == GOAL:
                optimal_policy[-1].append('G')
                continue
            bestAction = np.argmax(q_value[i,j,:])
            if bestAction == ACTION_UP:
                optimal_policy[-1].append('U')
            elif bestAction == ACTION_DOWN:
                optimal_policy[-1].append('D')
            elif bestAction == ACTION_LEFT:
                optimal_policy[-1].append('L')
            elif bestAction == ACTION_RIGHT:
                optimal_policy[-1].append('R')
    print('optimal policy is :')
    for row in optimal_policy:
        print(row)
    print('Wind strength for each column:\n{}'.format([str(w) for w in WIND]))

if __name__ == '__main__':
    sara()







